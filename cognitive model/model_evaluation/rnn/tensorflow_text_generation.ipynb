{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data.txt', 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pocgkr\\ncgwpcfks\\nkfsfkspwg\\npfkswkfsococg\\nwptgkfsoc\\npwfkg\\npowcer\\ncoprerk\\nwopofkkskf\\nopwpwgcoc\\npocrfksfks\\ncgpogpoc\\npwocgfkfsrere\\nocwgrk\\nwpgsfk\\npwofsg\\npocpocwgkfs\\npocgwg\\npwgockfsfksret\\nocpwgfkskfsrere\\ncopwgwpresk\\npwterfkf\\npocwggpfkfksrer\\nwpofcgkrpwgfksocr\\ncwpfksor\\npwfksorc\\npocgwkfkfser\\npowctgcog\\npowgcpo\\npwfkt\\npwokfkrg\\ncopcogpgwfksr\\npgsfkskfg\\nwgptocopwg\\npowcgre\\nperkscwgtf\\npcgworkfkfs\\npopocgkr\\npwtgcfkfksr\\npocwkf\\npgkfkf\\ncwopg\\npowcgkfkfr\\nwpgfksk\\npocwcpoere\\npwkf\\ncptgo\\npocfkswgr\\npowcfkfk\\npwgfksfk\\ngpwfks\\ncgwpw\\ncwopgtrekfksksf\\npocsfkw\\nocpwgfkf\\npwgterkfsoc\\npocgwrkfs\\ntgcwpro\\npocgkfskfr\\npgcow\\nopwg\\nwpgcowpcfksre\\nre\\nptwgfks\\npocrfkfk\\npockr\\npockfskfswgre\\ntgpcowpgpwcog\\ngwpfrkst\\npfkfksg\\npwogsfkg\\npwkf\\ngwpocoskfsfk\\nwtpcockfkf\\nwpgfkfkre\\npococwtrfkfkg\\npwocfksfksr\\npwgerks\\npoctfkfkrwg\\npwfksocr\\ntnpzc\\ntnpcz\\ntnpzc\\ntnzpc\\npmz\\nxmbzd\\npImz\\nmzxp\\npbz\\npgz\\npnzga\\ndnzgdp\\npndz\\nnzpdgds\\nmcddxp\\npcfkdddaag\\ndagpk\\npc\\ngkfadp\\npfgds\\nbcsfgkap\\npbsfkcga\\npbgafkcs\\ngpbcksfa\\npbfkscga\\npbsgkafc\\npbcskfga\\npsfkagcb\\npfkscagb\\npfskagcb\\npskfcagb\\npfkscagb\\npsfkgacb\\npfkbscga\\npcgafkbs\\ngpabkfsc\\npabkgcfs\\ngcpabkfs\\npasbkfgc\\npabcgksf\\nsfkpcgab\\nspgbkfca\\nspakgfcb\\ncgbkpasf\\nfkspcbag\\npkfsbgca\\ncpkfgsba\\ncbgsfapk\\ncgpsakfb\\npfksgcab\\npcskfbga\\nsbkpfcag\\nfpkgbsac\\npabfgsck\\ncpgfksab\\ngfcspkob\\npkfsbagc\\npkfsbgac\\npbfkscga\\nbpksfcga\\npagbkfsc\\npcgafksb\\npcsbfkga\\npbskfgca\\npscgfbak\\npfkcbgsa\\npkfsgcba\\npcfksagb\\npbcfskga\\npbfkcsga\\npcsfkbga\\npkfsabgc\\npkfsgcba\\npkfsgcba\\npkfsgcab\\npkfbscag\\npkbsfgca\\npkfbscga\\npkfsbgac\\npkfsagbc\\npkfsgcab\\npkfsgcba\\npkfsgcab\\npkfsacgb\\npkfsbcga\\npfskbcga\\npbkfscga\\npfskbgca\\nhhsdb\\nhhhdsgb\\nhhsgdb\\nhhdsbg\\nhhsgdb\\nhhdsgb\\ndsgbhh\\nhhsdbg\\nhhsdgb\\nhhsgdb\\nhhsgdb\\nhhsdgb\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = sorted(set(data))\n",
    "#vocabulary.remove('\\n')\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'h', b'z']]>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xhz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[4, 5, 6, 7, 8, 9, 10], [21, 11, 22]]>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'h', b'z']]>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xhz'], dtype=object)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training samples + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1592,), dtype=int64, numpy=array([16, 15,  6, ..., 10,  5,  2])>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(data, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\n",
      "o\n",
      "c\n",
      "g\n",
      "k\n",
      "r\n",
      "\n",
      "\n",
      "c\n",
      "g\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 8\n",
    "examples_per_epoch = len(data)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'p' b'o' b'c' b'g' b'k' b'r' b'\\n' b'c' b'g'], shape=(9,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'pocgkr\\ncg'\n",
      "b'wpcfks\\nkf'\n",
      "b'sfkspwg\\np'\n",
      "b'fkswkfsoc'\n",
      "b'ocg\\nwptgk'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'pocgkr\\nc'\n",
      "Target: b'ocgkr\\ncg'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 8), (64, 8)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 8, 23) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      multiple                  5888      \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  23575     \n",
      "=================================================================\n",
      "Total params: 3,967,767\n",
      "Trainable params: 3,967,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'kfsrere\\n'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'hrdmk\\nb[UNK]'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 8, 23)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         3.1347582\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.983078"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 3.1147\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 3.0136\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 2.8285\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.7495\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.6261\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 2.6008\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 2.5083\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.4412\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 2.3299\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.3020\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.1898\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.1799\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 2.1046\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 2.0947\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 2.0030\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.9875\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.9188\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.8673\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.8815\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.8684\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.8014\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.7736\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.7620\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.7273\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.7021\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.6844\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.6166\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.6245\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.6117\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.5989\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.5663\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.5231\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.5738\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.4834\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.4531\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.4274\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.4187\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.3719\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.3511\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.3759\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.3642\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.3183\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.3001\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.2537\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.2807\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.1889\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.1765\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.1433\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.1310\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.1314\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.0994\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.0587\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0388\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0042\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9781\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9163\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9082\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9024\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8556\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8286\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.8125\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.7763\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.7538\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.7438\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.7147\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.7134\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6806\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6527\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6221\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6267\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6073\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5886\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5853\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5871\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5647\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5558\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5339\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5470\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5368\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5239\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5322\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5162\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5002\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5032\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4996\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4780\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4651\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4884\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4713\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4692\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4517\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4505\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4529\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4429\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4563\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4400\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4556\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4352\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4392\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4450\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature=temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices = skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "        predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptocopwg\n",
      "pwg\n",
      "pwgf\n",
      "gpwg\n",
      "pwg\n",
      "pwgfkg\n",
      "pwpcogsf\n",
      "pwkf\n",
      "ggp\n",
      "pbz\n",
      "pfkskfg\n",
      "gpss\n",
      "gcsp\n",
      "pbsfksg\n",
      "pagb\n",
      "psfksr\n",
      "pgsb\n",
      "gc\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['p'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prequential method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[] for line in data_split]\n",
    "\n",
    "for line in range(0, len(data_split)):\n",
    "    states = None\n",
    "    next_char = tf.constant([data_split[line][0]])\n",
    "    results[line] = [next_char]\n",
    "    \n",
    "    for n in range(0, len(data_split[line])):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "        results[line].append(next_char)\n",
    "    results[line] = tf.strings.join(results[line])\n",
    "    results[line] = results[line][0].numpy().decode('utf-8')\n",
    "    results[line] = results[line].replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['powcgre',\n",
       " 'cagbpfks',\n",
       " 'kfspopoc',\n",
       " 'powcgrepwkfg',\n",
       " 'wtpcockfkf',\n",
       " 'pocgwk',\n",
       " 'pwgerk',\n",
       " 'copkfsp',\n",
       " 'wgkfspocop',\n",
       " 'opwgwpgcf',\n",
       " 'pwgfkfpwkf',\n",
       " 'cpocrfks',\n",
       " 'pwogwkfkfpgkr',\n",
       " 'ocgwpt',\n",
       " 'wcgfks',\n",
       " 'ptwocop',\n",
       " 'ptocopwgpwg',\n",
       " 'pocgks',\n",
       " 'pocgwkfkfksrpo',\n",
       " 'opwgwpgcfksgpo',\n",
       " 'cagbpfsksre',\n",
       " 'pocgwkfkf',\n",
       " 'pagbfkspkgpwg',\n",
       " 'wgkfspocoprgsfpo',\n",
       " 'cpocrfks',\n",
       " 'ptocopwg',\n",
       " 'poscgrepwga',\n",
       " 'powcgkfkfk',\n",
       " 'powcgre',\n",
       " 'ptocop',\n",
       " 'pwgfsfpo',\n",
       " 'copkfsgpopocw',\n",
       " 'pabgpbcsf',\n",
       " 'wkfpwogcgr',\n",
       " 'pwogsfkg',\n",
       " 'pocgwkfkfks',\n",
       " 'pocgwkfkfksr',\n",
       " 'ptgopocf',\n",
       " 'pocgwrpocwk',\n",
       " 'pocgkr',\n",
       " 'pabgfk',\n",
       " 'cgabs',\n",
       " 'poccgkfkfsc',\n",
       " 'wgkfspo',\n",
       " 'pwgfkfpwkf',\n",
       " 'pmzc',\n",
       " 'cwkfp',\n",
       " 'pzctnzpcz',\n",
       " 'pzterksc',\n",
       " 'pwkfgpwo',\n",
       " 'gwcpwf',\n",
       " 'cfkddd',\n",
       " 'cwgkfspocwcpoec',\n",
       " 'ptgopoc',\n",
       " 'owpgpwcog',\n",
       " 'powcgkfkfksf',\n",
       " 'pockfkfks',\n",
       " 'tgkfwpg',\n",
       " 'pocgkrcgpw',\n",
       " 'pasfb',\n",
       " 'ocwgr',\n",
       " 'wpfkscgpbbp',\n",
       " 're',\n",
       " 'pwoksfkg',\n",
       " 'powcgrep',\n",
       " 'pocgkr',\n",
       " 'ptocopwgpocgw',\n",
       " 'tgcfksdkxpgbc',\n",
       " 'gcbapcfk',\n",
       " 'powcgkfk',\n",
       " 'pwkfgwpo',\n",
       " 'powcg',\n",
       " 'gcabpkfbsgp',\n",
       " 'wpcfkskfp',\n",
       " 'wpcfkskfs',\n",
       " 'poscgkfpwoggr',\n",
       " 'pocpwocgpg',\n",
       " 'powcgkfk',\n",
       " 'pocgkrcgpwp',\n",
       " 'pzgwpcfk',\n",
       " 'tpococ',\n",
       " 'tpococ',\n",
       " 'terkfs',\n",
       " 'tpwoco',\n",
       " 'ptoc',\n",
       " 'xbpks',\n",
       " 'pocgk',\n",
       " 'mcab',\n",
       " 'powc',\n",
       " 'powc',\n",
       " 'pocgwk',\n",
       " 'dshhsg',\n",
       " 'pwogs',\n",
       " 'nzpdpp',\n",
       " 'mcpImz',\n",
       " 'pcopwfktr',\n",
       " 'dsbhs',\n",
       " 'poc',\n",
       " 'gcoapk',\n",
       " 'pwgfkf',\n",
       " 'bagcpkfs',\n",
       " 'powcgrep',\n",
       " 'pocgkrcg',\n",
       " 'gcospfsb',\n",
       " 'pwkfgwpo',\n",
       " 'pzstgcoc',\n",
       " 'pwfgsorp',\n",
       " 'pwkfgwpo',\n",
       " 'pwogsfkg',\n",
       " 'ptokfkfkg',\n",
       " 'pwkfgwpo',\n",
       " 'ptgopocf',\n",
       " 'pocgwkfks',\n",
       " 'pocgwkfkf',\n",
       " 'ptocopwg',\n",
       " 'gcbapkfs',\n",
       " 'pocgkrcg',\n",
       " 'gdbhhdsg',\n",
       " 'powcgrep',\n",
       " 'pcopwkf',\n",
       " 'sckghhsg',\n",
       " 'skfgpabf',\n",
       " 'shfsbhhs',\n",
       " 'cdbhhssd',\n",
       " 'fwpgfkfk',\n",
       " 'powcgkfkf',\n",
       " 'cskfgpcf',\n",
       " 'cgabspgb',\n",
       " 'cgabspgb',\n",
       " 'pwogsfkg',\n",
       " 'pocgwkfkf',\n",
       " 'sfkspwgp',\n",
       " 'fksacgpc',\n",
       " 'pocgfkwp',\n",
       " 'cagbpfsk',\n",
       " 'gsebbcga',\n",
       " 'powcgrep',\n",
       " 'pkobpkfs',\n",
       " 'powcgrep',\n",
       " 'bagcpkfs',\n",
       " 'powcgkfkf',\n",
       " 'pcobpkfb',\n",
       " 'pwogpogs',\n",
       " 'pwfgsoer',\n",
       " 'powcgrep',\n",
       " 'ptgkfkpw',\n",
       " 'pwkfwgpo',\n",
       " 'powcgkfkf',\n",
       " 'pocgwpkf',\n",
       " 'ptwocopwg',\n",
       " 'pocgtrkfs',\n",
       " 'pocgtrkfs',\n",
       " 'pzcgkfs',\n",
       " 'pzgtpocw',\n",
       " 'powcgkfkf',\n",
       " 'pwofkgpw',\n",
       " 'pocpwocg',\n",
       " 'ptocfkse',\n",
       " 'pwogsfkg',\n",
       " 'pocgwkfkf',\n",
       " 'pocgwkfkf',\n",
       " 'pzbtnzpc',\n",
       " 'pzctnpzc',\n",
       " 'pocgwkfkf',\n",
       " 'pwkfgwpo',\n",
       " 'pwgfkfpw',\n",
       " 'pwogsfkg',\n",
       " 'pocgkrcg',\n",
       " 'hhsdbg',\n",
       " 'hsdbghh',\n",
       " 'hhdsbg',\n",
       " 'hhsdbg',\n",
       " 'hhsdbg',\n",
       " 'hhsdbg',\n",
       " 'dpImz',\n",
       " 'hsdgbh',\n",
       " 'hhdsbg',\n",
       " 'hsdbgh',\n",
       " 'hhddbg',\n",
       " 'hsdbgh']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
